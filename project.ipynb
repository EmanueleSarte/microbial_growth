{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emcee\n",
    "import corner\n",
    "import pickle\n",
    "import inspect\n",
    "\n",
    "from IPython.display import display, Math\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from evolver import Evolver\n",
    "from models import Model3\n",
    "from util_funs import find_initial_params, plot_chains, print_latex_result, read_files,\\\n",
    "    filter_data, filter_file_names, get_run_id\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# True if you want to read from the dataset files, False if you want to generate the date with the model\n",
    "USE_DATASET = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_DATASET:\n",
    "    # m0, w1, w2, u, v = 0.5, 0.7, 3, 0.1, 2\n",
    "    # model = Model1_2(m0=m0, w1=w1, w2=w2, u=u, v=v)\n",
    "    # model.summary(show_h=False)\n",
    "\n",
    "    # m0, w1, w2, u, v = 0.5, 2, 3, 0.8, 20\n",
    "    # model = Model1_2(m0=m0, w1=w1, w2=w2, u=u, v=v)\n",
    "    # model.summary(show_h=False)\n",
    "\n",
    "    # Here we define our model\n",
    "    a, b, c, d, m_f_i, w2, u, v = 4, 0.3, 3, 10, 5, 0.8, 2, 7\n",
    "    model = Model3(a=a, b=b, c=c, d=d, m_f=m_f_i, w2=w2, u=u, v=v)\n",
    "    alpha = model.alpha\n",
    "    k = model.k\n",
    "\n",
    "    # Plot the model\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    model.get_figure(axs[0])\n",
    "\n",
    "\n",
    "    # Here we make the model evolve (and divide)\n",
    "    n_generations = 400\n",
    "    evolver = Evolver(model=model)\n",
    "    evolver.evolve(n_generations, debug=False)\n",
    "\n",
    "    # Here we show some of the result\n",
    "    end_gen = 30\n",
    "    time, data = evolver.get_data(0, end_gen)\n",
    "    final_time, final_data = evolver.get_start_data(0, end_gen)\n",
    "\n",
    "    ptime = evolver.time[evolver.offset_start[0] : evolver.offset_end[end_gen] + 1]\n",
    "    params = evolver.params[evolver.offset_start[0] : evolver.offset_end[end_gen] + 1]\n",
    "\n",
    "    axs[1].plot(time, data[:, 0])\n",
    "    axs[1].set_title('Microbe Size')\n",
    "    axs[1].plot(final_time, final_data[:, 0], marker = 'o', linestyle = '');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the data generated from our model to estimate the parameters \n",
    "\n",
    "if not USE_DATASET:\n",
    "    truth = [a, b, c, d, w2, u, v] # our parameters defined above\n",
    "    labels = ['a', 'b', 'c', 'd', 'w2', 'u', 'v']\n",
    "\n",
    "    nwalkers = 25 # number of walkers for the MCMC\n",
    "    # We add some gaussian noise to our initial parameters\n",
    "    guess = np.array(truth) + 1e-4 * np.random.randn(nwalkers, len(truth)) \n",
    "\n",
    "    # This is still Model3 specific\n",
    "    spans = evolver.spans\n",
    "    alphas = evolver.get_alphas()[:-1]\n",
    "    kappas = evolver.get_kappas()[:-1]\n",
    "    m_finals = evolver.get_mfinals()[:-1]\n",
    "\n",
    "\n",
    "    from multiprocessing import Pool\n",
    "    with Pool() as pool:\n",
    "        sampler = emcee.EnsembleSampler(nwalkers, len(truth), evolver.model.log_prob,\n",
    "                                        args=(spans, alphas, kappas, m_finals), pool=pool)\n",
    "        sampler.run_mcmc(guess, 5000, progress=True)\n",
    "\n",
    "    samples = sampler.get_chain()\n",
    "    plot_chains(samples, title=\"TITOLO\", labels=labels)\n",
    "\n",
    "    tau = None\n",
    "    try:\n",
    "        tau = sampler.get_autocorr_time()\n",
    "        print(\"Auto correlation time:\", tau)\n",
    "    except emcee.autocorr.AutocorrError as e:\n",
    "        print(\"The chain is too short to get the auto correlation time\")\n",
    "\n",
    "    if tau is not None:\n",
    "        thin_number = int(np.mean(tau) / 2)\n",
    "        print(f\"The thinning is {thin_number}, calculated from the auto correlation time\")\n",
    "    else:\n",
    "        thin_number = 35\n",
    "        print(f\"The thinning is {thin_number}, the default value\")\n",
    "\n",
    "    flat_samples = sampler.get_chain(discard=500, thin=thin_number, flat=True)\n",
    "    print(\"Flat samples shape: \", flat_samples.shape)\n",
    "\n",
    "    fig = corner.corner(flat_samples, labels=labels, truths=truth)\n",
    "    fig.suptitle(f\"TITOLO\")\n",
    "    plt.show()\n",
    "\n",
    "    result_dict = {}\n",
    "    for i in range(len(labels)):\n",
    "        mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "        q = np.diff(mcmc)\n",
    "        result_dict[labels[i]] = (mcmc[1], q[0], q[1])\n",
    "        \n",
    "    print_latex_result(list(result_dict.values()), labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TO READ AND WRITE THE DATASET ###################\n",
    "\n",
    "tanouchi25c_set = pd.read_csv(\"./dataset/Tanouchi25C.csv\")\n",
    "tanouchi37c_set = pd.read_csv(\"./dataset/Tanouchi37C.csv\")\n",
    "susman18_set = pd.read_csv(\"./dataset/Susman18_physical_units.csv\")\n",
    "tanouchi25c_set.df_name = \"tanouchi25c\"\n",
    "tanouchi37c_set.df_name = \"tanouchi37c\"\n",
    "susman18_set.df_name = \"susman18\"\n",
    "\n",
    "names_to_dataset_map = {d.df_name: d for d in [tanouchi25c_set, tanouchi37c_set, susman18_set]}\n",
    "\n",
    "\n",
    "START = False\n",
    "if USE_DATASET and START:\n",
    "\n",
    "    df = susman18_set\n",
    "    DF_NAME = df.df_name\n",
    "\n",
    "    columns = [\"growth_rate\"]\n",
    "    df, df_deleted = filter_data(df, filter_cols=columns, show=False, remove_lineages=True)\n",
    "\n",
    "    print(f\"Eliminated {len(df_deleted)} rows\")\n",
    "\n",
    "    WRITE_ON_FILE = True\n",
    "    OUTPUT = True\n",
    "    AUTO_FIND_P0 = True\n",
    "    FOLDER_PATH = \"./results/\"\n",
    "\n",
    "    lineages = []\n",
    "\n",
    "    a = 104.197\n",
    "    b = 0.007\n",
    "    c = 53.654\n",
    "    d = 64.138\n",
    "    w2 = 1.832\n",
    "    u = 0.928\n",
    "    v = 2.461\n",
    "\n",
    "    if AUTO_FIND_P0:\n",
    "        truths = find_initial_params(df)\n",
    "    else:\n",
    "        truths = [a, b, c, d, w2, u, v]\n",
    "\n",
    "    labels = ['a', 'b', 'c', 'd', 'w2', 'u', 'v']\n",
    "\n",
    "    nwalkers = 25\n",
    "    ndim = len(truths)\n",
    "    guess = np.array(truths) + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "    model = Model3()\n",
    "    run_id = get_run_id(model.__class__)\n",
    "\n",
    "    if not lineages:\n",
    "        lineages = list(df['lineage_ID'].unique().astype(int))\n",
    "\n",
    "    df = df[df[\"lineage_ID\"].isin(lineages)]\n",
    "\n",
    "    with open(FOLDER_PATH + f\"run_{run_id}.txt\", \"w\") as f:\n",
    "        f.write(inspect.getsource(Model3().log_prior))\n",
    "        \n",
    "\n",
    "    print(f\"######## ANALYZING DATASET {DF_NAME}, RUN_ID: {run_id} ########\")\n",
    "    print(\"Initial Parameters:\")\n",
    "    print(\" \".join([f\"{l}={v:.3f}\" for v, l in zip(truths, labels)]))\n",
    "    print(\"\")\n",
    "\n",
    "    for (lin_id,), df_lin in df.groupby([\"lineage_ID\"]):\n",
    "        print(f\"DATASET {DF_NAME}, LINEAGE: {lin_id}, n data: {len(df_lin)}\")\n",
    "\n",
    "        spans = df_lin[\"generationtime\"][1:].to_numpy()\n",
    "        alphas = df_lin[\"growth_rate\"][1:].to_numpy()\n",
    "        kappas = df_lin[\"division_ratio\"][1:].to_numpy()\n",
    "        m_finals = df_lin[\"length_final\"][:-1].to_numpy()\n",
    "\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(nwalkers, ndim, model.log_prob,\n",
    "                                            args=(spans, alphas, kappas, m_finals), pool=pool)\n",
    "            sampler.run_mcmc(guess, 3000, progress=True)\n",
    "\n",
    "        # sampler = emcee.EnsembleSampler(nwalkers, ndim, model.log_prob,\n",
    "        #                                 args=(spans, alphas, kappas, m_finals))\n",
    "        # sampler.run_mcmc(guess, 3000, progress=True)\n",
    "\n",
    "        samples = sampler.get_chain()\n",
    "\n",
    "        if OUTPUT:\n",
    "            plot_chains(samples, title=f\"{DF_NAME}  run_id: {run_id}  lineage: {lin_id}\", labels=labels)\n",
    "\n",
    "        tau = None\n",
    "        try:\n",
    "            tau = sampler.get_autocorr_time()\n",
    "            print(\"Auto correlation time:\", tau)\n",
    "        except emcee.autocorr.AutocorrError as e:\n",
    "            print(\"The chain is too short to get the auto correlation time\")\n",
    "\n",
    "        if tau is not None:\n",
    "            thin_number = int(np.mean(tau) / 2)\n",
    "            print(f\"The thinning is {thin_number}, calculated from the auto correlation time\")\n",
    "        else:\n",
    "            thin_number = 20\n",
    "            print(f\"The thinning is {thin_number}, the default value\")\n",
    "\n",
    "        discard = 1000\n",
    "        print(f\"The discard value is {discard}\")\n",
    "        flat_samples = sampler.get_chain(discard=discard, thin=thin_number, flat=True)\n",
    "        print(\"Flat samples shape: \", flat_samples.shape)\n",
    "        if OUTPUT:\n",
    "            fig = corner.corner(flat_samples, labels=labels, truths=truths)\n",
    "            fig.suptitle(f\"{DF_NAME} lineage: {lin_id}\")\n",
    "            plt.show()\n",
    "\n",
    "        result_dict = {}\n",
    "        for i in range(ndim):\n",
    "            mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "            q = np.diff(mcmc)\n",
    "            result_dict[labels[i]] = (mcmc[1], q[0], q[1])\n",
    "\n",
    "        if OUTPUT:\n",
    "            print_latex_result(list(result_dict.values()), labels)\n",
    "\n",
    "        if WRITE_ON_FILE:\n",
    "            data = {\"n_walkers\": nwalkers, \"ndim\": ndim, \"dataset\": DF_NAME, \"lineage_id\": lin_id,\n",
    "                    \"truths\": truths, \"initial_guess\": guess, \"labels\": labels,\n",
    "                    \"spans\": spans, \"alphas\": alphas, \"kappas\": kappas, \"m_finals\": m_finals,\n",
    "                    \"samples\": samples, \"tau\": tau, \"thin_number\": thin_number, \"discard\": discard,\n",
    "                    \"flat_samples\": flat_samples, \"result_dict\": result_dict, \"data_length\": len(df_lin),\n",
    "                    \"data\": {\"spans\": spans, \"alphas\": alphas, \"kappas\": kappas, \"m_finals\": m_finals},\n",
    "                    \"run_id\": run_id\n",
    "                    }\n",
    "\n",
    "            filename = f\"{DF_NAME}_run_{run_id:06}_lin_{int(lin_id):03}.bin\"\n",
    "            Path(FOLDER_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            with open(FOLDER_PATH + filename, \"wb\") as f:\n",
    "                pickle.dump(data, f)\n",
    "\n",
    "            print(\"File created: \", FOLDER_PATH + filename)\n",
    "            print(\"----------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### READ BACK THE FILE CREATED ########################\n",
    "FOLDER_PATH = \"./results/\"\n",
    "\n",
    "# lineages = [1,2]\n",
    "# dataset_results_dict = read_files(FOLDER_PATH, df_name=\"tan\", run_id=301954,lineages=lineages)\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "# for _, data in dataset_results_dict.items():\n",
    "#     df_name, lin_id = data[\"dataset\"], data[\"lineage_id\"]\n",
    "#     labels, samples, flat_samples = data[\"labels\"], data[\"samples\"], data[\"flat_samples\"]\n",
    "#     result_dict, truths = data[\"result_dict\"], data[\"truths\"]\n",
    "\n",
    "#     title = f\"DATASET {df_name}, lineage {lin_id}\"\n",
    "\n",
    "#     print(title)\n",
    "#     plot_chains(samples, title=title, labels=labels)\n",
    "#     fig = corner.corner(flat_samples, labels=labels, truths=truths)\n",
    "#     fig.suptitle(title)\n",
    "#     plt.show()\n",
    "#     print_latex_result(list(result_dict.values()), labels)\n",
    "#     print(\"-------------------------------\")\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\n",
    "lineages = []\n",
    "dataset_results_dict = read_files(FOLDER_PATH, df_name=\"tan\", run_id=301954,lineages=lineages)\n",
    "\n",
    "ROWS, COLS = 3, 3\n",
    "fig, axs = plt.subplots(ROWS, COLS, figsize=(COLS * 4, ROWS * 4))\n",
    "\n",
    "for i, data in enumerate(dataset_results_dict.values()):\n",
    "    df_name, lin_id = data[\"dataset\"], data[\"lineage_id\"]\n",
    "    labels, flat_samples = data[\"labels\"], data[\"flat_samples\"]\n",
    "    result_dict, truths = data[\"result_dict\"], data[\"truths\"]\n",
    "\n",
    "    # recall parameter order = a, b, c, d, w2, u, v\n",
    "\n",
    "    # for j in range(flat_samples.shape[1]):\n",
    "    #     nbins = int(np.sqrt(flat_samples[:, j].shape[0]))\n",
    "    #     index = (j // COLS, j % COLS)\n",
    "    #     if (index == (0,2) or index == (1,0)): continue\n",
    "    #     axs[index].hist(flat_samples[:, j], alpha=0.7, bins=nbins, label=f\"Lin {lin_id}\", density=True)\n",
    "\n",
    "    nbins = int(np.sqrt(flat_samples.shape[0]))\n",
    "\n",
    "    axs[0, 0].hist(flat_samples[:, 0], label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "    \n",
    "    axs[0, 1].hist(flat_samples[:, 1], label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "    \n",
    "    w1 = flat_samples[:, 0] * flat_samples[:, 1]\n",
    "    axs[0, 2].hist(w1, label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "    \n",
    "    sigma_w1 = np.sqrt(flat_samples[:, 0]) * flat_samples[:, 1]\n",
    "    axs[1, 0].hist(sigma_w1, label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "\n",
    "    axs[1, 1].hist(flat_samples[:, 4], label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "    \n",
    "    axs[1, 2].hist(flat_samples[:, 4] / w1, label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "    \n",
    "    axs[2, 0].hist(flat_samples[:, 5], label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "    \n",
    "    axs[2, 1].hist(flat_samples[:, 6], label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "    \n",
    "    axs[2, 2].hist(flat_samples[:, 5] / flat_samples[:, 6], label=f\"Lin {lin_id}\", alpha=0.7,\n",
    "            bins=nbins, density=True)\n",
    "    \n",
    "    \n",
    "\n",
    "for j, label in enumerate(labels):\n",
    "    nbins = int(np.sqrt(flat_samples[:, j].shape[0]))\n",
    "    index = (j // COLS, j % COLS)\n",
    "    axs[index].set_title(label)\n",
    "\n",
    "fig.suptitle(f\"DATASET {df_name}, #data: {len(dataset_results_dict)}\")\n",
    "axs[0, 0].set_title('a')\n",
    "axs[0, 1].set_title('b')\n",
    "axs[0, 2].set_title('w1 = a * b')\n",
    "axs[1, 0].set_title('sigma_w1 = sqrt(a) * b')\n",
    "axs[1, 1].set_title('w2')\n",
    "axs[1, 2].set_title('w2 / w1')\n",
    "axs[2, 0].set_title('u')\n",
    "axs[2, 1].set_title('v')\n",
    "axs[2, 2].set_title('u / v');\n",
    "\n",
    "axs[1, 0].set_title(\"w2 / w1\")\n",
    "axs[2, 1].set_title(\"u / v\");\n",
    "axs[2, 2].set_title(\"w1 = a * b\");\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "# ROWS, COLS = 2, 2\n",
    "\n",
    "# for i, (_, data) in enumerate(dataset_results_dict.items()):\n",
    "#     df_name, lin_id = data[\"dataset\"], data[\"lineage_id\"]\n",
    "#     if int(lin_id) not in [1, 2, 3] or not df_name.startswith(\"tan\"):\n",
    "#         continue\n",
    "\n",
    "#     print(\"Compatible file: \", df_name, \" lineage: \", lin_id)\n",
    "#     labels, flat_samples = data[\"labels\"], data[\"flat_samples\"]\n",
    "#     results, truths = data[\"result_dict\"], data[\"truths\"]\n",
    "#     data_length = data[\"data_length\"]\n",
    "#     # spans = data[\"data\"][\"spans\"]\n",
    "#     # alphas = data[\"data\"][\"alphas\"]\n",
    "#     # kappas = data[\"data\"][\"kappas\"]\n",
    "#     # m_finals = data[\"data\"][\"m_finals\"]\n",
    "\n",
    "#     params = {\"a\":results[\"a\"][0], \"b\":results[\"b\"][0], \"c\":results[\"c\"][0], \"d\":results[\"d\"][0],\n",
    "#               \"m_f\":1, \"w2\":results[\"w2\"][0], \"u\":results[\"u\"][0], \"v\":results[\"v\"][0]}\n",
    "#     # print(params)\n",
    "#     model_test = Model3(**params)\n",
    "#     params_txt = \" \".join([f\"{k}={v:.3g}\" for k, v in model_test.params_dict.items()])\n",
    "\n",
    "#     evolver_test = Evolver(model_test)\n",
    "#     evolver_test.evolve(n_div=data_length)\n",
    "\n",
    "#     fig, axs = plt.subplots(ROWS, COLS, figsize=(COLS * 4, ROWS * 4))\n",
    "#     model_data = [evolver_test.spans, evolver_test.get_alphas(), evolver_test.get_kappas(), evolver_test.get_mfinals()]\n",
    "#     for j, (name, value) in enumerate(data[\"data\"].items()):\n",
    "#         nbins = int(np.sqrt(len(model_data[j])))\n",
    "\n",
    "#         index = (j // COLS, j % COLS)\n",
    "#         axs[index].hist(model_data[j], label=\"Generated\", density=True, alpha=0.7, bins=nbins)\n",
    "#         axs[index].hist(value, label=\"Lineage\", density=True, alpha=0.7, bins=nbins)\n",
    "#         axs[index].set_title(name)\n",
    "#         axs[index].legend()\n",
    "    \n",
    "#     fig.suptitle(f\"{DF_NAME} lineage: {lin_id}, params: {params_txt}\", wrap=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = tanouchi25c_set\n",
    "# df = tanouchi37c_set\n",
    "# df = susman18_set\n",
    "df = susman18_set\n",
    "run_id = \"734846\"\n",
    "\n",
    "\n",
    "acmin = 1\n",
    "acmax = 20 + 1\n",
    "lineages = [*np.arange(1, 10)]\n",
    "fig, axs = plt.subplots(2, 1, figsize=(18, 14))\n",
    "# plt.acorr\n",
    "plt.suptitle(f\"{df.df_name}\")\n",
    "axs[0].hlines([0], acmin, acmax - 1, colors=\"black\", lw=1)\n",
    "axs[1].hlines([0], acmin, acmax - 1, colors=\"black\", lw=1)\n",
    "axs[0].set_title(\"Alphas Autocorrelations\")\n",
    "axs[1].set_title(\"Kappas Autocorrelations\")\n",
    "\n",
    "lineages_passed = [lin for _, _, lin in filter_file_names(FOLDER_PATH, df.df_name, run_id).values()]\n",
    "\n",
    "sums = np.zeros(shape=(2, acmax - acmin))\n",
    "n_data = 0\n",
    "include_exc = False\n",
    "for (lin_id,), df_lin in df.groupby([\"lineage_ID\"]):\n",
    "    # print(f\"DATASET {DF_NAME}, LINEAGE: {lin_id}, n data: {len(df_lin)}\")\n",
    "    if lineages and lin_id not in lineages:\n",
    "        continue\n",
    "\n",
    "    ac_alpha_list = []\n",
    "    ac_kappa_list = []\n",
    "    for i in range(acmin, acmax):\n",
    "        a = df_lin[\"growth_rate\"][:-i]\n",
    "        b = df_lin[\"growth_rate\"][i:]\n",
    "        ac_alpha_list.append(np.corrcoef(a, b)[0, 1])\n",
    "\n",
    "        c = df_lin[\"division_ratio\"][1:][:-i]\n",
    "        d = df_lin[\"division_ratio\"][1:][i:]\n",
    "        ac_kappa_list.append(np.corrcoef(c, d)[0, 1])\n",
    "\n",
    "    passed = int(lin_id) in lineages_passed\n",
    "    if include_exc or passed:\n",
    "        sums[0, :] += ac_alpha_list\n",
    "        sums[1, :] += ac_kappa_list\n",
    "        n_data += 1\n",
    "\n",
    "    x_range = np.arange(acmin, acmax)\n",
    "    ls = \"solid\" if passed else \"dashed\"\n",
    "    axs[0].plot(x_range, ac_alpha_list, label=f\"Lin {lin_id}\", ls=ls, lw=1)\n",
    "    axs[1].plot(x_range, ac_kappa_list, label=f\"Lin {lin_id}\", ls=ls, lw=1)\n",
    "    # plt.scatter(x_range, autocorre_list, s=10)\n",
    "\n",
    "sums = sums/n_data\n",
    "text_mean = \"With excluded\" if include_exc else \"Without excluded\"\n",
    "axs[0].plot(x_range, sums[0, :], ls=\"dotted\", color=\"black\", lw=2, label=f\"Mean ({text_mean})\")\n",
    "axs[1].plot(x_range, sums[1, :], ls=\"dotted\", color=\"black\", lw=2, label=f\"Mean ({text_mean})\")\n",
    "axs[0].legend()\n",
    "axs[1].legend();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1dc0716793a58acb4a53f754119613b942ffd643eacae86bee36f7354dd7ff1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
